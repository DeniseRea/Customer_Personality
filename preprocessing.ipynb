{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef757fb6",
   "metadata": {},
   "source": [
    "# Customer Personality Analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835cd422",
   "metadata": {},
   "source": [
    "## 1. Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caec037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f29db0",
   "metadata": {},
   "source": [
    "## 2. Descargar y Cargar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar dataset desde Kaggle\n",
    "path = kagglehub.dataset_download(\"imakash3011/customer-personality-analysis\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3310ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "import os\n",
    "files = os.listdir(path)\n",
    "print(\"Archivos disponibles:\", files)\n",
    "\n",
    "# Cargar el CSV\n",
    "df = pd.read_csv(os.path.join(path, 'marketing_campaign.csv'), sep='\\t')\n",
    "print(f\"\\nDataset cargado: {df.shape[0]} filas y {df.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ade76",
   "metadata": {},
   "source": [
    "## 3. Exploración Inicial de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b130cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeras filas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd6f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información general\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas descriptivas\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores nulos\n",
    "print(\"Valores nulos por columna:\")\n",
    "null_counts = df.isnull().sum()\n",
    "null_percentage = (null_counts / len(df)) * 100\n",
    "null_df = pd.DataFrame({'Nulos': null_counts, 'Porcentaje': null_percentage})\n",
    "print(null_df[null_df['Nulos'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46612c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores duplicados\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Filas duplicadas: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e204c9",
   "metadata": {},
   "source": [
    "## 4. Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0224dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia para preprocesamiento\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(f\"Dataset original: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejar valores nulos en Income (imputar con mediana)\n",
    "if 'Income' in df_clean.columns:\n",
    "    median_income = df_clean['Income'].median()\n",
    "    df_clean['Income'].fillna(median_income, inplace=True)\n",
    "    print(f\"Valores nulos en Income imputados con la mediana: {median_income}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa39bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados\n",
    "df_clean.drop_duplicates(inplace=True)\n",
    "print(f\"Dataset después de eliminar duplicados: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff43dab6",
   "metadata": {},
   "source": [
    "## 5. Ingeniería de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d7bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir Dt_Customer a datetime y calcular antigüedad\n",
    "if 'Dt_Customer' in df_clean.columns:\n",
    "    df_clean['Dt_Customer'] = pd.to_datetime(df_clean['Dt_Customer'], format='%d-%m-%Y')\n",
    "    reference_date = df_clean['Dt_Customer'].max()\n",
    "    df_clean['Customer_Days'] = (reference_date - df_clean['Dt_Customer']).dt.days\n",
    "    df_clean['Customer_Years'] = df_clean['Customer_Days'] / 365.25\n",
    "    print(\"Antigüedad del cliente calculada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular edad a partir del año de nacimiento\n",
    "if 'Year_Birth' in df_clean.columns:\n",
    "    current_year = datetime.now().year\n",
    "    df_clean['Age'] = current_year - df_clean['Year_Birth']\n",
    "    \n",
    "    # Eliminar outliers de edad (menores de 18 o mayores de 100)\n",
    "    df_clean = df_clean[(df_clean['Age'] >= 18) & (df_clean['Age'] <= 100)]\n",
    "    print(f\"Edad calculada. Dataset después de filtrar outliers: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e41b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular total de hijos\n",
    "if 'Kidhome' in df_clean.columns and 'Teenhome' in df_clean.columns:\n",
    "    df_clean['Total_Children'] = df_clean['Kidhome'] + df_clean['Teenhome']\n",
    "    df_clean['Has_Children'] = (df_clean['Total_Children'] > 0).astype(int)\n",
    "    print(\"Total de hijos calculado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d203bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular gasto total\n",
    "spending_columns = ['MntWines', 'MntFruits', 'MntMeatProducts', \n",
    "                   'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "\n",
    "if all(col in df_clean.columns for col in spending_columns):\n",
    "    df_clean['Total_Spending'] = df_clean[spending_columns].sum(axis=1)\n",
    "    print(\"Gasto total calculado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d781ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular total de compras\n",
    "purchase_columns = ['NumWebPurchases', 'NumCatalogPurchases', \n",
    "                   'NumStorePurchases', 'NumDealsPurchases']\n",
    "\n",
    "if all(col in df_clean.columns for col in purchase_columns):\n",
    "    df_clean['Total_Purchases'] = df_clean[purchase_columns].sum(axis=1)\n",
    "    print(\"Total de compras calculado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular tasa de aceptación de campañas\n",
    "campaign_columns = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', \n",
    "                   'AcceptedCmp4', 'AcceptedCmp5', 'Response']\n",
    "\n",
    "if all(col in df_clean.columns for col in campaign_columns):\n",
    "    df_clean['Total_Campaigns_Accepted'] = df_clean[campaign_columns].sum(axis=1)\n",
    "    print(\"Total de campañas aceptadas calculado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f057e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular gasto promedio por compra\n",
    "if 'Total_Spending' in df_clean.columns and 'Total_Purchases' in df_clean.columns:\n",
    "    df_clean['Avg_Spending_Per_Purchase'] = df_clean['Total_Spending'] / (df_clean['Total_Purchases'] + 1)\n",
    "    print(\"Gasto promedio por compra calculado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca471a82",
   "metadata": {},
   "source": [
    "## 6. Transformación de Variables Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baebef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar columnas categóricas\n",
    "categorical_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "print(\"Columnas categóricas:\")\n",
    "for col in categorical_columns:\n",
    "    print(f\"\\n{col}: {df_clean[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf91bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplificar Education\n",
    "if 'Education' in df_clean.columns:\n",
    "    education_mapping = {\n",
    "        'Graduation': 'Graduate',\n",
    "        'PhD': 'Postgraduate',\n",
    "        'Master': 'Postgraduate',\n",
    "        'Basic': 'Undergraduate',\n",
    "        '2n Cycle': 'Undergraduate'\n",
    "    }\n",
    "    df_clean['Education_Level'] = df_clean['Education'].map(education_mapping)\n",
    "    print(\"\\nEducation simplificado:\")\n",
    "    print(df_clean['Education_Level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplificar Marital_Status\n",
    "if 'Marital_Status' in df_clean.columns:\n",
    "    marital_mapping = {\n",
    "        'Married': 'Partner',\n",
    "        'Together': 'Partner',\n",
    "        'Single': 'Single',\n",
    "        'Divorced': 'Single',\n",
    "        'Widow': 'Single',\n",
    "        'Alone': 'Single',\n",
    "        'Absurd': 'Single',\n",
    "        'YOLO': 'Single'\n",
    "    }\n",
    "    df_clean['Relationship_Status'] = df_clean['Marital_Status'].map(marital_mapping)\n",
    "    print(\"\\nMarital_Status simplificado:\")\n",
    "    print(df_clean['Relationship_Status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dummies para variables categóricas\n",
    "df_encoded = df_clean.copy()\n",
    "\n",
    "if 'Education_Level' in df_encoded.columns:\n",
    "    education_dummies = pd.get_dummies(df_encoded['Education_Level'], prefix='Education')\n",
    "    df_encoded = pd.concat([df_encoded, education_dummies], axis=1)\n",
    "\n",
    "if 'Relationship_Status' in df_encoded.columns:\n",
    "    relationship_dummies = pd.get_dummies(df_encoded['Relationship_Status'], prefix='Relationship')\n",
    "    df_encoded = pd.concat([df_encoded, relationship_dummies], axis=1)\n",
    "\n",
    "print(f\"\\nDataset con variables dummy: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94b81c",
   "metadata": {},
   "source": [
    "## 7. Detección y Tratamiento de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f0750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar outliers en Income\n",
    "if 'Income' in df_clean.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Boxplot\n",
    "    axes[0].boxplot(df_clean['Income'])\n",
    "    axes[0].set_title('Boxplot de Income')\n",
    "    axes[0].set_ylabel('Income')\n",
    "    \n",
    "    # Histogram\n",
    "    axes[1].hist(df_clean['Income'], bins=50, edgecolor='black')\n",
    "    axes[1].set_title('Distribución de Income')\n",
    "    axes[1].set_xlabel('Income')\n",
    "    axes[1].set_ylabel('Frecuencia')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a861a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar outliers usando IQR para Income\n",
    "if 'Income' in df_clean.columns:\n",
    "    Q1 = df_clean['Income'].quantile(0.25)\n",
    "    Q3 = df_clean['Income'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    print(f\"Income - Límite inferior: {lower_bound}, Límite superior: {upper_bound}\")\n",
    "    \n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[(df_clean['Income'] >= lower_bound) & (df_clean['Income'] <= upper_bound)]\n",
    "    after = len(df_clean)\n",
    "    \n",
    "    print(f\"Outliers eliminados: {before - after}\")\n",
    "    print(f\"Dataset final: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b14f7b",
   "metadata": {},
   "source": [
    "## 8. Normalización y Escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Seleccionar columnas numéricas para escalar\n",
    "numeric_columns = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Excluir columnas que no necesitan escalado\n",
    "columns_to_exclude = ['ID', 'Year_Birth', 'Z_CostContact', 'Z_Revenue']\n",
    "columns_to_scale = [col for col in numeric_columns if col not in columns_to_exclude]\n",
    "\n",
    "print(f\"Columnas a escalar: {len(columns_to_scale)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset con StandardScaler\n",
    "df_standardized = df_clean.copy()\n",
    "scaler_standard = StandardScaler()\n",
    "\n",
    "df_standardized[columns_to_scale] = scaler_standard.fit_transform(df_clean[columns_to_scale])\n",
    "print(\"StandardScaler aplicado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7141b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset con MinMaxScaler\n",
    "df_normalized = df_clean.copy()\n",
    "scaler_minmax = MinMaxScaler()\n",
    "\n",
    "df_normalized[columns_to_scale] = scaler_minmax.fit_transform(df_clean[columns_to_scale])\n",
    "print(\"MinMaxScaler aplicado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae33b0",
   "metadata": {},
   "source": [
    "## 9. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"RESUMEN DEL PREPROCESAMIENTO\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nDataset original: {df.shape}\")\n",
    "print(f\"Dataset limpio: {df_clean.shape}\")\n",
    "print(f\"Dataset con encoding: {df_encoded.shape}\")\n",
    "print(f\"\\nFilas eliminadas: {df.shape[0] - df_clean.shape[0]}\")\n",
    "print(f\"Nuevas columnas creadas: {df_clean.shape[1] - df.shape[1]}\")\n",
    "print(f\"\\nValores nulos restantes: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417cc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar correlación\n",
    "plt.figure(figsize=(20, 16))\n",
    "correlation_matrix = df_clean[columns_to_scale].corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Matriz de Correlación de Variables Numéricas', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82063745",
   "metadata": {},
   "source": [
    "## 10. Guardar Datos Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc37902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar datasets procesados\n",
    "df_clean.to_csv('customer_data_clean.csv', index=False)\n",
    "df_encoded.to_csv('customer_data_encoded.csv', index=False)\n",
    "df_standardized.to_csv('customer_data_standardized.csv', index=False)\n",
    "df_normalized.to_csv('customer_data_normalized.csv', index=False)\n",
    "\n",
    "print(\"Datasets guardados exitosamente:\")\n",
    "print(\"- customer_data_clean.csv\")\n",
    "print(\"- customer_data_encoded.csv\")\n",
    "print(\"- customer_data_standardized.csv\")\n",
    "print(\"- customer_data_normalized.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
